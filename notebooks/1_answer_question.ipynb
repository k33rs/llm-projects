{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# Answer technical questions with Frontier and Open Source models\n",
    "\n",
    "This tool that takes a technical question, and responds with an explanation. This demo shows how to invoke both OpenAI's Frontier models and Meta's Open Source models, deployed locally with Ollama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display, update_display\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad014cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key looks good!\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "if api_key and api_key.startswith('sk-proj-') and len(api_key)>10:\n",
    "    print(\"API key looks good!\")\n",
    "else:\n",
    "    print(\"There might be a problem with your API key?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment\n",
    "openai = OpenAI(api_key=api_key)\n",
    "# first run \"ollama run llama3.2\" to start the local server\n",
    "ollama = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64d4a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt_default = \"You are a helpful programming assistant.\"\n",
    "user_prompt_default = \"\"\"Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\"\n",
    "\n",
    "sys_prompt = input(\"Enter system prompt: \") or sys_prompt_default\n",
    "user_prompt = input(\"Enter user prompt: \") or user_prompt_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df4d0446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_response(model_name, response, stream):\n",
    "    \"\"\"Display the response from the model.\"\"\"\n",
    "    if stream:\n",
    "        try:\n",
    "            res_stream = \"\"\n",
    "            display_id = display(Markdown(f\"*{model_name} is typing...*\"), display_id=True).display_id\n",
    "\n",
    "            for chunk in response:\n",
    "                delta = chunk.choices[0].delta.content or \"\"\n",
    "                res_stream += delta\n",
    "                update_display(Markdown(f\"*{model_name} is typing...*\\n\\n{res_stream}\"), display_id=display_id)\n",
    "\n",
    "            update_display(Markdown(f\"*{model_name}'s answer:*\\n\\n{res_stream}\"), display_id=display_id)\n",
    "        except Exception as e:\n",
    "            update_display(Markdown(f\"*Error during streaming:*\\n{e}\"), display_id=display_id)\n",
    "    else:\n",
    "        try:\n",
    "            display(Markdown(f\"*{model_name}'s answer:*\\n\\n{response.choices[0].message.content}\"))\n",
    "        except Exception as e:\n",
    "            print(f\"Error when trying to get a response from the model:\")\n",
    "            print(f\"model name: {model_name}\\n{e}\")\n",
    "\n",
    "\n",
    "def ask_model_and_display(client, model_name, sys_prompt, user_prompt, stream=False):\n",
    "    \"\"\"Ask the model and display the response, either streamed or not.\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": sys_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ],\n",
    "            stream=stream\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Error when trying to get a response from the model:\")\n",
    "        print(f\"model name: {model_name}\\n{e}\")\n",
    "    else:\n",
    "        display_response(model_name, response, stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "*gpt-4o-mini's answer:*\n",
       "\n",
       "The provided code is a Python expression that uses a generator with the `yield from` statement to yield values from a set comprehension. Let's break it down step by step:\n",
       "\n",
       "1. **Set Comprehension**: \n",
       "   ```python\n",
       "   {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "   ```\n",
       "   - This creates a set containing all unique authors from a collection of `books`.\n",
       "   - It iterates over a list of `books` (which is expected to be an iterable), and for each `book` in this list, it calls `book.get(\"author\")` to get the author's name.\n",
       "   - The `if book.get(\"author\")` condition ensures that only books with a non-null or non-empty author are included in the set. This means if a book does not have an author listed (`None` or an empty string), it is skipped.\n",
       "\n",
       "2. **Generator with `yield from`**:\n",
       "   ```python\n",
       "   yield from ...\n",
       "   ```\n",
       "   - The `yield from` statement is used to yield all values from an iterable. In this case, it is used to yield each unique author obtained from the set comprehension.\n",
       "\n",
       "### Summary:\n",
       "Overall, this code snippet is a concise way to gather all unique authors from a list of books and yield them one by one. It's useful for scenarios where you want to process or access each author individually in a lazy manner (i.e., one at a time), rather than creating a complete list of authors in memory. \n",
       "\n",
       "### Why Use This Approach:\n",
       "- **Efficiency**: By using a set comprehension, it automatically handles duplicates (only unique authors are stored).\n",
       "- **Lazy Evaluation**: Using `yield from` allows for on-the-fly generation of authors, which can be more memory-efficient, especially when dealing with large datasets.\n",
       "- **Clarity**: The use of comprehensions and `yield from` provides a clear and concise way to write the code, making it easier to understand for those familiar with Python."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "ask_model_and_display(openai, MODEL_GPT, sys_prompt, user_prompt, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9328d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama 3.2 with streaming\n",
    "ask_model_and_display(ollama, MODEL_LLAMA, sys_prompt, user_prompt, stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472a5e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama 3.2 without streaming\n",
    "ask_model_and_display(ollama, MODEL_LLAMA, sys_prompt, user_prompt, stream=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "portfolio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
